{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based approach\n",
    "\n",
    "NLTK is short for Natural Language ToolKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's play around with VADER\n",
    "\n",
    "We'll first import VADER from within the 'sentiment' module of NLTK.\n",
    "This will display a warning, saying that we don't have the `twython` library installed, but that's OK. We won't be using that library.\n",
    "\n",
    "Remember that VADER is a lexicon and a *rule-based* sentiment analysis tool. VADER is *NOT* using machine learning.\n",
    "\n",
    "We'll take advantage of the VADER `SentimentIntensityAnalyzer` to do our calculations for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Update from demo:* Apologies. Apparently I had installed literally every tool and library available for jupyter notebook, one of them included vader_lexicon, which is required to use the `vader.SentimentIntensityAnalyzer()` mentioned below. To download this package uncomment and execute the following line. In the window that appears (might appear behind this browser window) choose the `models` tab, and **WHATEVER YOU DO, DO NOT SCROLL USING A SCROLL WHEEL OR A TOUCH PAD**. Use the scroll bar on the right to find `vader_lexicon`, choose install, and close the window.\n",
    "\n",
    "You only need to do this once so once that's installed, you can safely comment out the below line again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roger/anaconda/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import vader\n",
    "vanalyser = vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll introduce a helper function to easily see what VADER thinks of a problem instance.\n",
    "\n",
    "This function will return four values:\n",
    "- negativity (neg)\n",
    "- neutrality (neu)\n",
    "- positivity (pos)\n",
    "- compound: The 'total' score, calculated using a [non-trivial algorithm](http://stackoverflow.com/q/40325980)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vanalyse(sample):\n",
    "    return vanalyser.polarity_scores(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out some sentences, and see what VADER thinks of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': -0.4767, 'neg': 0.608, 'neu': 0.392, 'pos': 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\"What a terrible restaurant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experts predict that the sentiment of the sentence \"What a terrible restaurant\" was mostly negative, with some neutral, and no positive sentiment. Fairly accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that VADER undestands emoticons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.5106, 'neg': 0.0, 'neu': 0.0, 'pos': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\":D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not necessarily all emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\"-,-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It understands how punctuation and capitalisation acts as boosters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.4404, 'neg': 0.0, 'neu': 0.508, 'pos': 0.492}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\"the food was good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.4926, 'neg': 0.0, 'neu': 0.484, 'pos': 0.516}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\"the food was good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.6027, 'neg': 0.0, 'neu': 0.433, 'pos': 0.567}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\"the food was GOOD!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It even understands double-negatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.5096, 'neg': 0.0, 'neu': 0.603, 'pos': 0.397}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\"the food was not the worst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is not flawless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.3291, 'neg': 0.234, 'neu': 0.398, 'pos': 0.368}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\"I usually hate seafood, but I liked this\") #Percieved as positive (True positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': -0.2263, 'neg': 0.352, 'neu': 0.381, 'pos': 0.267}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanalyse(\"I usually hate seafood, and I liked this\") #Percieved as negative (False negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using VADER on our dataset\n",
    "\n",
    "Let's see how VADER does over a large dataset of reviews where we know the polarity of each problem instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is import the data. If you are on a mac/linux machine, the below commands should just work. If you're on a Windows machine you might have to change the forward slashes in the strings below to *two* backslashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Technical note: The gorram encoding is latin1, not UTF-8!\n",
    "with open(\"./data/negative-reviews.txt\", \"r\", encoding=\"latin1\") as file:\n",
    "    negativeReviews = file.readlines()\n",
    "with open(\"./data/positive-reviews.txt\", \"r\", encoding=\"latin1\") as file:\n",
    "    positiveReviews = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the reviews in two lists - one list for the negative reviews, one for the positive reviews.\n",
    "\n",
    "Let's create another helper function that only returns the compound (total) score of the VADER analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vaderSentiment(review):\n",
    "    return vanalyse(review)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the method works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4767"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaderSentiment(\"Adam Sandler is a terrible actor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider how we'll measure the quality of our models, keeping in mind that we'll be implementing another model using ML later. \n",
    "\n",
    "Let's create a function whose input is some sort of function that calculates the score for each review in each of the lists of reviews. This function will return a dictionary, (hash map) containing two keys: 'pos' and 'neg', mapping to the list of the scores for the positive and negative lists respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getReviewSentiments(sentimentCalculator):\n",
    "    \"\"\"\n",
    "    Given a function that calculates sentiment of a reviews list, return list of sentiment.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Python syntax explainer:\n",
    "    #For each review in positiveReviews we assign each review to a variable 'posReview'.\n",
    "    #Calculate the sentiment of 'posReview', and put it in the 'positiveResults' list.\n",
    "    #Repeat for all positive reviews.\n",
    "    #Do the same for the negative reviews.\n",
    "\n",
    "    positiveResults = [sentimentCalculator(posReview) for posReview in positiveReviews]\n",
    "    negativeResults = [sentimentCalculator(negReview) for negReview in negativeReviews]\n",
    "    return {'pos': positiveResults, 'neg': negativeResults}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to evaluate how well our models are doing, we need a way of measuring this. Let's create a function that prints the percentage of the reviews a model. If we write our code carefully we can then reuse this function when we start implementing an ML solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runDiagnostics(reviewSentiments):\n",
    "    positiveReviews = reviewSentiments['pos']\n",
    "    negativeReviews = reviewSentiments['neg']\n",
    "\n",
    "    #How many reviews are the for each kind?\n",
    "    numberOfNegative = len(negativeReviews)\n",
    "    numberOfPositive = len(positiveReviews)\n",
    "    numberOfReviews = numberOfNegative + numberOfPositive\n",
    "    \n",
    "    #How many reviews were correct for each kind?\n",
    "    totalTruePositives = float(sum(x > 0 for x in positiveReviews))\n",
    "    totalTrueNegatives = float(sum(x < 0 for x in negativeReviews))\n",
    "\n",
    "    #Convert to percentages\n",
    "    pctTotalAccurate = (totalTruePositives + totalTrueNegatives) * 100 / numberOfReviews\n",
    "    pctTruePositive = totalTruePositives * 100 / numberOfPositive\n",
    "    pctTrueNegative = totalTrueNegatives * 100 / numberOfNegative\n",
    "\n",
    "    #Print, and format percentages to have 2 decimal digits\n",
    "    print(\"Accuracy on positive reviews = \" + \"%.2f\" %(pctTruePositive) + \"%\")\n",
    "    print(\"Accuracy on negative reviews = \" + \"%.2f\" %(pctTrueNegative) + \"%\")\n",
    "    print(\"Overall accuracy = \" + \"%.2f\" %(pctTotalAccurate) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment of truth: *How well does VADER do?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on positive reviews = 69.46%\n",
      "Accuracy on negative reviews = 40.11%\n",
      "Overall accuracy = 54.78%\n"
     ]
    }
   ],
   "source": [
    "reviewSentiments = getReviewSentiments(vaderSentiment)\n",
    "runDiagnostics(reviewSentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To paraphrase the late Hans Rosling: Overall only slightly better than chimpanzees.\n",
    "\n",
    "VADER has decent accuracy when it comes to positive reviews, but a chimpanzee is better at determining if a review is negative or not.\n",
    "\n",
    "It is important that our model is equally good at identifying true positives and true negatives. The VADER model appears to be biased towards giving positive scores. Not good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the luxury of having pretty clean data (no dupes, same number of positive and negative reviews). Which allows us to go almost straight into writing the training step! We just need to do a bit for reformatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we're doing is splitting the data into a training set and a test set. First of all, let's see how many reviews we have in each list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n"
     ]
    }
   ],
   "source": [
    "print(len(positiveReviews))\n",
    "print(len(negativeReviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for simplicity, let's use 2500 reviews of each list as our training set. That is, only ~47% of our data is used to train the model. Usually this number is around 60-80%. If this makes you uneasy, we'll go back and change it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitIndex = 2500\n",
    "\n",
    "trainingPositiveReviews = positiveReviews[:splitIndex]\n",
    "trainingNegativeReviews = negativeReviews[:splitIndex]\n",
    "\n",
    "testPositiveReviews = positiveReviews[splitIndex+1:]\n",
    "testNegativeReviews = negativeReviews[splitIndex+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our vocabulary. This is the list of all unique words present in our *training data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVocabulary():\n",
    "    #Get list of all words (incl. repetition) contained in the positive reviews.\n",
    "    #Repeat for the negative reviews.\n",
    "    positiveWordList = [word for line in trainingPositiveReviews for word in line.split()]\n",
    "    negativeWordList = [word for line in trainingNegativeReviews for word in line.split()]\n",
    "    \n",
    "    #Combine the words in to one big list\n",
    "    allWordList = [item for sublist in [positiveWordList, negativeWordList] for item in sublist]\n",
    "    \n",
    "    #Remove duplicates, and return\n",
    "    return list(set(allWordList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well now, how big is our vocabulary? As a reference, if a person has a vocabulary of ~10,000 words this person is by many considered fluent in English. That being said, capitalisation and punctuation significantly increases the number in our vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14094"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = getVocabulary()\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually we want to pass the training data to an algorithm owned by NLTK. This data needs to have a specific format, so let's create a function that transforms the data into the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainingData():\n",
    "    negTaggedTrainingReviewList = [{'review':oneReview.split(), 'label': 'negative'} for oneReview in trainingNegativeReviews]\n",
    "    posTaggedTrainingReviewList = [{'review':oneReview.split(), 'label': 'positive'} for oneReview in trainingPositiveReviews]\n",
    "    \n",
    "    fullTaggedTrainingData = [item for sublist in [negTaggedTrainingReviewList, posTaggedTrainingReviewList] for item in sublist]\n",
    "    \n",
    "    #Technical note: A list of (sampleValues, label) tuples,\n",
    "    #  where sampleValues is a list of each individual word in the sample.\n",
    "    return [(review['review'], review['label']) for review in fullTaggedTrainingData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'negative')\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "trainingData = getTrainingData()\n",
    "print(trainingData[0])\n",
    "print(len(trainingData)) #Should = 2 * splitIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first parameter is a vector of all the words in our problem instance, and the second parameter is the label 'positive' or 'negative'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFeatures(review):\n",
    "    #Remove duplicates from the review.\n",
    "    #Considerations: is this the right thing to do?\n",
    "    #Yes: We are using Naive Bayes, and two identical words among the features\n",
    "    #would violate the independence assumption.\n",
    "    #No: Repeating a word often acts as a booster:\n",
    "    #E.g. \"Never ever ever ever watch this film\"\n",
    "    reviewWords = set(review)\n",
    "\n",
    "    features = {}\n",
    "    for word in vocabulary:\n",
    "        features[word]=(word in reviewWords)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual training of our model. This is very simple in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainedNaiveBayesClassifier(extractFeatures, trainingData):\n",
    "    #Turn the training data into a list of feature vectors\n",
    "    trainingFeatures = nltk.classify.apply_features(extractFeatures, trainingData) \n",
    "    return nltk.NaiveBayesClassifier.train(trainingFeatures);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next bit takes quite a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainedNBClassifier = getTrainedNaiveBayesClassifier(extractFeatures, trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now trained our classifier. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naiveBayesSentimentCalculator(review):\n",
    "    problemInstance = review.split();\n",
    "    problemFeatures = extractFeatures(problemInstance)\n",
    "    return trainedNBClassifier.classify(problemFeatures)\n",
    "\n",
    "naiveBayesSentimentCalculator(\"great film\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naiveBayesSentimentCalculator(\"adam sandler sucks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a test harness so we can check the entire test dataset:\n",
    "def getTestReviewSentiments(naiveBayesSentimentCalculator):\n",
    "    testNegResults = [naiveBayesSentimentCalculator(review) for review in testNegativeReviews]\n",
    "    testPosResults = [naiveBayesSentimentCalculator(review) for review in testPositiveReviews]\n",
    "    \n",
    "    labelToNum = {'positive':1, 'negative':-1}\n",
    "    \n",
    "    numericNegResults = [labelToNum[x] for x in testNegResults]\n",
    "    numericPosResults = [labelToNum[x] for x in testPosResults]\n",
    "    \n",
    "    return {'pos': numericPosResults, 'neg': numericNegResults}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on positive reviews = 73.39%\n",
      "Accuracy on negative reviews = 77.03%\n",
      "Overall accuracy = 75.21%\n"
     ]
    }
   ],
   "source": [
    "runDiagnostics(getTestReviewSentiments(naiveBayesSentimentCalculator)) #Takes a while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's it we're screwed. The machines will soon be our new rulers.\n",
    "The simple Naive Bayes model we just trained significantly outperformed VADER, and that lexicon was made by human experts.\n",
    "\n",
    "...right?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
